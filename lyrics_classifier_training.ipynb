{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xP11wMuDeFEW",
        "outputId": "898c1b88-6ed9-421f-ba55-8009ed6fbaa7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "\n",
        "lyrics = []\n",
        "label = []\n",
        "title = []\n",
        "artist = []\n",
        "\n",
        "with open('/content/drive/MyDrive/CSV/clean_indonesia_song_lyrics_emotion_datas.csv', mode='r' , encoding='utf-8') as file:\n",
        "    reader = csv.DictReader(file)\n",
        "    for doc in reader:\n",
        "      lyrics.append(doc['lyrics'])\n",
        "      label.append(int(doc['label']))\n",
        "      title.append(doc['song_name'])\n",
        "      artist.append(doc['artist'])\n",
        "\n"
      ],
      "metadata": {
        "id": "7ToChNPQeaN7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json"
      ],
      "metadata": {
        "id": "cmxQx0pFTINd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "list_lyrics = {\n",
        "    \"list_key\" : [],\n",
        "    \"data\" : {\n",
        "\n",
        "    }\n",
        "}"
      ],
      "metadata": {
        "id": "ZEAMowiNUOWH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(title)):\n",
        "  list_lyrics[\"list_key\"].append(f\"{title[i]} - {artist[i]}\")\n",
        "  list_lyrics[\"data\"][f\"{title[i]} - {artist[i]}\"] = stemming_lyrics[i]\n"
      ],
      "metadata": {
        "id": "PhZ7qpHmTLdB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('lyrics_list.json', 'w') as file:\n",
        "    json.dump(list_lyrics, file, indent=4)"
      ],
      "metadata": {
        "id": "7b9D4MilVsGo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install Sastrawi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R3GymV5vewI1",
        "outputId": "c6596d0e-85d8-48d7-e91e-d408339b5a0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting Sastrawi\n",
            "  Downloading Sastrawi-1.0.1-py2.py3-none-any.whl.metadata (909 bytes)\n",
            "Downloading Sastrawi-1.0.1-py2.py3-none-any.whl (209 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/209.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m \u001b[32m204.8/209.7 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.7/209.7 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: Sastrawi\n",
            "Successfully installed Sastrawi-1.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory"
      ],
      "metadata": {
        "id": "qPiovKfie3Wh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def case_folding(list_doc):\n",
        "  temp = []\n",
        "  for doc in list_doc:\n",
        "    temp.append(doc.lower())\n",
        "  return temp\n",
        "\n",
        "def normalization(list_doc):\n",
        "  temp = []\n",
        "  for doc in list_doc:\n",
        "    temp.append(' '.join(re.findall(r'[a-zA-Z]+', doc)))\n",
        "  return temp\n",
        "\n",
        "def tokenization(list_doc):\n",
        "  temp = []\n",
        "  for doc in list_doc:\n",
        "    temp.append(doc.split())\n",
        "  return temp\n",
        "\n",
        "def stopword_removal(list_of_tokenize_doc):\n",
        "  result = []\n",
        "  stopwords = [\n",
        "    \"ada\", \"adalah\", \"adanya\", \"adapun\", \"agak\", \"agaknya\", \"agar\", \"akan\", \"akankah\", \"akhir\", \"akhiri\", \"akhirnya\", \"aku\", \"akulah\", \"amat\", \"amatlah\", \"anda\", \"andalah\", \"antar\", \"antara\", \"antaranya\", \"apa\", \"apaan\", \"apabila\", \"apakah\", \"apalagi\", \"apatah\", \"artinya\", \"asal\", \"asalkan\", \"atas\", \"atau\", \"ataukah\", \"ataupun\", \"awal\", \"awalnya\", \"bagai\", \"bagaikan\", \"bagaimana\", \"bagaimanakah\", \"bagaimanapun\", \"bagi\", \"bagian\", \"bahkan\", \"bahwa\", \"bahwasanya\", \"baik\", \"bakal\", \"bakalan\", \"balik\", \"banyak\", \"bapak\", \"baru\", \"bawah\", \"beberapa\", \"begini\", \"beginian\", \"beginikah\", \"beginilah\", \"begitu\", \"begitukah\", \"begitulah\", \"begitupun\", \"bekerja\", \"belakang\", \"belakangan\", \"belum\", \"belumlah\", \"benar\", \"benarkah\", \"benarlah\", \"berada\", \"berakhir\", \"berakhirlah\", \"berakhirnya\", \"berapa\", \"berapakah\", \"berapalah\", \"berapapun\", \"berarti\", \"berawal\", \"berbagai\", \"berdatangan\", \"beri\", \"berikan\", \"berikut\", \"berikutnya\", \"berjumlah\", \"berkali-kali\", \"berkata\", \"berkehendak\", \"berkeinginan\", \"berkenaan\", \"berlainan\", \"berlalu\", \"berlangsung\", \"berlebihan\", \"bermacam\", \"bermacam-macam\", \"bermaksud\", \"bermula\", \"bersama\", \"bersama-sama\", \"bersiap\", \"bersiap-siap\", \"bertanya\", \"bertanya-tanya\", \"berturut\", \"berturut-turut\", \"bertutur\", \"berujar\", \"berupa\", \"besar\", \"betul\", \"betulkah\", \"biasa\", \"biasanya\", \"bila\", \"bilakah\", \"bisa\", \"bisakah\", \"boleh\", \"bolehkah\", \"bolehlah\", \"buat\", \"bukan\", \"bukankah\", \"bukanlah\", \"bukannya\", \"bulan\", \"bung\", \"cara\", \"caranya\", \"cukup\", \"cukupkah\", \"cukuplah\", \"cuma\", \"dahulu\", \"dalam\", \"dan\", \"dapat\", \"dari\", \"daripada\", \"datang\", \"dekat\", \"demi\", \"demikian\", \"demikianlah\", \"dengan\", \"depan\", \"di\", \"dia\", \"diakhiri\", \"diakhirinya\", \"dialah\", \"diantara\", \"diantaranya\", \"diberi\", \"diberikan\", \"diberikannya\", \"dibuat\", \"dibuatnya\", \"didapat\", \"didatangkan\", \"digunakan\", \"diibaratkan\", \"diibaratkannya\", \"diingat\", \"diingatkan\", \"diinginkan\", \"dijawab\", \"dijelaskan\", \"dijelaskannya\", \"dikarenakan\", \"dikatakan\", \"dikatakannya\", \"dikerjakan\", \"diketahui\", \"diketahuinya\", \"dikira\", \"dilakukan\", \"dilalui\", \"dilihat\", \"dimaksud\", \"dimaksudkan\", \"dimaksudkannya\", \"dimaksudnya\", \"diminta\", \"dimintai\", \"dimisalkan\", \"dimulai\", \"dimulailah\", \"dimulainya\", \"dimungkinkan\", \"dini\", \"dipastikan\", \"diperbuat\", \"diperbuatnya\", \"dipergunakan\", \"diperkirakan\", \"diperlihatkan\", \"diperlukan\", \"diperlukannya\", \"dipersoalkan\", \"dipertanyakan\", \"dipunyai\", \"diri\", \"dirinya\", \"disampaikan\", \"disebut\", \"disebutkan\", \"disebutkannya\", \"disini\", \"disinilah\", \"ditambahkan\", \"ditandaskan\", \"ditanya\", \"ditanyai\", \"ditanyakan\", \"ditegaskan\", \"ditujukan\", \"ditunjuk\", \"ditunjuki\", \"ditunjukkan\", \"ditunjukkannya\", \"ditunjuknya\", \"dituturkan\", \"dituturkannya\", \"diucapkan\", \"diucapkannya\", \"diungkapkan\", \"dong\", \"dua\", \"dulu\", \"empat\", \"enggak\", \"enggaknya\", \"entah\", \"entahlah\", \"guna\", \"gunakan\", \"hal\", \"hampir\", \"hanya\", \"hanyalah\", \"hari\", \"harus\", \"haruslah\", \"harusnya\", \"hendak\", \"hendaklah\", \"hendaknya\", \"hingga\", \"ia\", \"ialah\", \"ibarat\", \"ibaratkan\", \"ibaratnya\", \"ibu\", \"ikut\", \"ingat\", \"ingat-ingat\", \"ingin\", \"inginkah\", \"inginkan\", \"ini\", \"inikah\", \"inilah\", \"itu\", \"itukah\", \"itulah\", \"jadi\", \"jadilah\", \"jadinya\", \"jangan\", \"jangankan\", \"janganlah\", \"jauh\", \"jawab\", \"jawaban\", \"jawabnya\", \"jelas\", \"jelaskan\", \"jelaslah\", \"jelasnya\", \"jika\", \"jikalau\", \"juga\", \"jumlah\", \"jumlahnya\", \"justru\", \"kala\", \"kalau\", \"kalaulah\", \"kalaupun\", \"kalian\", \"kami\", \"kamilah\", \"kamu\", \"kamulah\", \"kan\", \"kapan\", \"kapankah\", \"kapanpun\", \"karena\", \"karenanya\", \"kasus\", \"kata\", \"katakan\", \"katakanlah\", \"katanya\", \"ke\", \"keadaan\", \"kebetulan\", \"kecil\", \"kedua\", \"keduanya\", \"keinginan\", \"kelamaan\", \"kelihatan\", \"kelihatannya\", \"kelima\", \"keluar\", \"kembali\", \"kemudian\", \"kemungkinan\", \"kemungkinannya\", \"kenapa\", \"kepada\", \"kepadanya\", \"kesampaian\", \"keseluruhan\", \"keseluruhannya\", \"keterlaluan\", \"ketika\", \"khususnya\", \"kini\", \"kinilah\", \"kira\", \"kira-kira\", \"kiranya\", \"kita\", \"kitalah\", \"kok\", \"kurang\", \"lagi\", \"lagian\", \"lah\", \"lain\", \"lainnya\", \"lalu\", \"lama\", \"lamanya\", \"lanjut\", \"lanjutnya\", \"lebih\", \"lewat\", \"lima\", \"luar\", \"macam\", \"maka\", \"makanya\", \"makin\", \"malah\", \"malahan\", \"mampu\", \"mampukah\", \"mana\", \"manakala\", \"manalagi\", \"masa\", \"masalah\", \"masalahnya\", \"masih\", \"masihkah\", \"masing\", \"masing-masing\", \"mau\", \"maupun\", \"melainkan\", \"melakukan\", \"melalui\", \"melihat\", \"melihatnya\", \"memang\", \"memastikan\", \"memberi\", \"memberikan\", \"membuat\", \"memerlukan\", \"memihak\", \"meminta\", \"memintakan\", \"memisalkan\", \"memperbuat\", \"mempergunakan\", \"memperkirakan\", \"memperlihatkan\", \"mempersiapkan\", \"mempersoalkan\", \"mempertanyakan\", \"mempunyai\", \"memulai\", \"memungkinkan\", \"menaiki\", \"menambahkan\", \"menandaskan\", \"menanti\", \"menanti-nanti\", \"menantikan\", \"menanya\", \"menanyai\", \"menanyakan\", \"mendapat\", \"mendapatkan\", \"mendatang\", \"mendatangi\", \"mendatangkan\", \"menegaskan\", \"mengakhiri\", \"mengapa\", \"mengatakan\", \"mengatakannya\", \"mengenai\", \"mengerjakan\", \"mengetahui\", \"menggunakan\", \"menghendaki\", \"mengibaratkan\", \"mengibaratkannya\", \"mengingat\", \"mengingatkan\", \"menginginkan\", \"mengira\", \"mengiranya\", \"mengucapkan\", \"mengucapkannya\", \"mengungkapkan\", \"menjadi\", \"menjawab\", \"menjelaskan\", \"menuju\", \"menunjuk\", \"menunjuki\", \"menunjukkan\", \"menunjuknya\", \"menurut\", \"menuturkan\", \"menyampaikan\", \"menyangkut\", \"menyatakan\", \"menyebutkan\", \"menyeluruh\", \"menyiapkan\", \"merasa\", \"mereka\", \"merekalah\", \"merupakan\", \"meski\", \"meskipun\", \"meyakini\", \"meyakinkan\", \"minta\", \"mirip\", \"misal\", \"misalkan\", \"misalnya\", \"mula\", \"mulai\", \"mulailah\", \"mulanya\", \"mungkin\", \"mungkinkah\", \"nah\", \"naik\", \"namun\", \"nanti\", \"nantinya\", \"nyaris\", \"nyatanya\", \"oleh\", \"olehnya\", \"pada\", \"padahal\", \"padanya\", \"pak\", \"paling\", \"panjang\", \"pantas\", \"para\", \"pasti\", \"pastilah\", \"penting\", \"pentingnya\", \"per\", \"percuma\", \"perlu\", \"perlukah\", \"perlunya\", \"pernah\", \"persoalan\", \"pertama\", \"pertama-tama\", \"pertanyaan\", \"pertanyakan\", \"pihak\", \"pihaknya\", \"pukul\", \"pula\", \"pun\", \"punya\", \"rasa\", \"rasanya\", \"rupa\", \"rupanya\", \"saat\", \"saatnya\", \"saja\", \"sajalah\", \"saling\", \"sama\", \"sama-sama\", \"sambil\", \"sampai\", \"sampai-sampai\", \"sampaikan\", \"sana\", \"sangat\", \"sangatlah\", \"satu\", \"saya\", \"sayalah\", \"se\", \"sebab\", \"sebabnya\", \"sebagai\", \"sebagaimana\", \"sebagainya\", \"sebagian\", \"sebaik\", \"sebaik-baiknya\", \"sebaiknya\", \"sebaliknya\", \"sebanyak\", \"sebegini\", \"sebegitu\", \"sebelum\", \"sebelumnya\", \"sebenarnya\", \"seberapa\", \"sebesar\", \"sebetulnya\", \"sebisanya\", \"sebuah\", \"sebut\", \"sebutlah\", \"sebutnya\", \"secara\", \"secukupnya\", \"sedang\", \"sedangkan\", \"sedemikian\", \"sedikit\", \"sedikitnya\", \"seenaknya\", \"segala\", \"segalanya\", \"segera\", \"seharusnya\", \"sehingga\", \"seingat\", \"sejak\", \"sejauh\", \"sejenak\", \"sejumlah\", \"sekadar\", \"sekadarnya\", \"sekali\", \"sekali-kali\", \"sekalian\", \"sekaligus\", \"sekalipun\", \"sekarang\", \"sekaranglah\", \"sekecil\", \"seketika\", \"sekiranya\", \"sekitar\", \"sekitarnya\", \"sekurang-kurangnya\", \"sekurangnya\", \"sela\", \"selain\", \"selaku\", \"selalu\", \"selama\", \"selama-lamanya\", \"selamanya\", \"selanjutnya\", \"seluruh\", \"seluruhnya\", \"semacam\", \"semakin\", \"semampu\", \"semampunya\", \"semasa\", \"semasih\", \"semata\", \"semata-mata\", \"semaunya\", \"sementara\", \"semisal\", \"semisalnya\", \"sempat\", \"semua\", \"semuanya\", \"semula\", \"sendiri\", \"sendirian\", \"sendirinya\", \"seolah\", \"seolah-olah\", \"seorang\", \"sepanjang\", \"sepantasnya\", \"sepantasnyalah\", \"seperlunya\", \"sepertinya\", \"sepihak\", \"sering\", \"seringnya\", \"serta\", \"serupa\", \"sesaat\", \"sesama\", \"sesampai\", \"sesegera\", \"sesekali\", \"seseorang\", \"sesuatu\", \"sesuatunya\", \"sesudah\", \"sesudahnya\", \"setelah\", \"setempat\", \"setengah\", \"seterusnya\", \"setiap\", \"setiba\", \"setibanya\", \"setidak-tidaknya\", \"setidaknya\", \"setinggi\", \"seusai\", \"sewaktu\", \"siap\", \"siapa\", \"siapakah\", \"siapapun\", \"sini\", \"sinilah\", \"soal\", \"soalnya\", \"suatu\", \"sudah\", \"sudahkah\", \"sudahlah\", \"supaya\", \"tadi\", \"tadinya\", \"tahu\", \"tahun\", \"tak\", \"tambah\", \"tambahnya\", \"tampak\", \"tampaknya\", \"tandas\", \"tandasnya\", \"tanpa\", \"tanya\", \"tanyakan\", \"tanyanya\", \"tapi\", \"tegas\", \"tegasnya\", \"telah\", \"tempat\", \"tengah\", \"tentang\", \"tentu\", \"tentulah\", \"tentunya\", \"tepat\", \"terakhir\", \"terasa\", \"terbanyak\", \"terberi\", \"terhadap\", \"terhadapnya\", \"terjadi\", \"terjadilah\", \"terjadinya\", \"terkira\", \"terlalu\", \"terlebih\", \"terlihat\", \"termasuk\", \"ternyata\", \"tersampaikan\", \"tersebut\", \"tersebutlah\", \"tertentu\", \"tertutup\", \"tertuju\", \"terus\", \"terutama\", \"tetap\", \"tetapi\", \"tiap\", \"tiba\", \"tiba-tiba\", \"tidak\", \"tidakkah\", \"tidaklah\", \"tinggi\", \"toh\", \"tunjuk\", \"turut\", \"tutur\", \"tuturnya\", \"ucap\", \"ucapnya\", \"ujar\", \"ujarnya\", \"umum\", \"umumnya\", \"ungkap\", \"ungkapnya\", \"untuk\", \"usah\", \"usai\", \"waduh\", \"wah\", \"wahai\", \"waktu\", \"waktunya\", \"walau\", \"walaupun\", \"wong\", \"yaitu\", \"yakin\", \"yakni\", \"yang\"\n",
        "  ]\n",
        "\n",
        "  for doc in list_of_tokenize_doc:\n",
        "    temp = []\n",
        "    for word in doc:\n",
        "      if word in stopwords:\n",
        "        continue\n",
        "      temp.append(word)\n",
        "    result.append(temp)\n",
        "  return result\n",
        "\n",
        "\n",
        "\n",
        "def stemming(list_of_tokenize_doc):\n",
        "  factory = StemmerFactory()\n",
        "  stemmer = factory.create_stemmer()\n",
        "  result = []\n",
        "  for doc in list_of_tokenize_doc:\n",
        "    temp = []\n",
        "    for word in doc:\n",
        "      temp.append(stemmer.stem(word))\n",
        "    result.append(temp)\n",
        "  return result\n"
      ],
      "metadata": {
        "id": "t_3np0Oeesp0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "case_folding_lyrics = case_folding(lyrics)\n",
        "normalization_lyrics = normalization(case_folding_lyrics)\n",
        "tokenization_lyrics = tokenization(normalization_lyrics)\n",
        "stopword_removal_lyrics = stopword_removal(tokenization_lyrics)"
      ],
      "metadata": {
        "id": "hD6So4cUe5mm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stemming_lyrics = stemming(stopword_removal_lyrics)"
      ],
      "metadata": {
        "id": "Fb5Xq_nYfGUA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "import numpy as np\n",
        "\n",
        "class TFIDF:\n",
        "    def __init__(self):\n",
        "        self.word_counter = None\n",
        "        self.index_to_word = None\n",
        "        self.word_to_index = None\n",
        "        self.document_frequency = None\n",
        "        self.n_docs = 0\n",
        "        self.n_vocab = 0\n",
        "\n",
        "    def _clean_data(self, data: list[str]) -> list[str]:\n",
        "        \"\"\"Cleans and preprocesses data by removing non-alphabet characters and lowering case.\"\"\"\n",
        "        return [' '.join(re.findall(r'[a-zA-Z]+', sentence)).lower() for sentence in data]\n",
        "\n",
        "    def _build_vocab(self, cleaned_data: list[str]) -> list[str]:\n",
        "        \"\"\"Builds vocabulary and mappings from cleaned data.\"\"\"\n",
        "        word_list = [word for sentence in cleaned_data for word in sentence.split()]\n",
        "        self.word_counter = dict(sorted(Counter(word_list).items()))\n",
        "        self.index_to_word = {idx: word for idx, word in enumerate(self.word_counter.keys())}\n",
        "        self.word_to_index = {word: idx for idx, word in enumerate(self.word_counter.keys())}\n",
        "        self.n_vocab = len(self.word_counter)\n",
        "\n",
        "    def _calculate_document_frequency(self, cleaned_data):\n",
        "        \"\"\"Calculates document frequency for each word.\"\"\"\n",
        "        self.document_frequency = {word: 0 for word in self.word_to_index.keys()}\n",
        "        for sentence in cleaned_data:\n",
        "          splitted_sentence = sentence.split(\" \")\n",
        "          unique_word = np.unique(splitted_sentence)\n",
        "          for word in unique_word:\n",
        "            self.document_frequency[word] += 1\n",
        "\n",
        "    def _calculate_tfidf(self, sentence):\n",
        "        \"\"\"Calculates the TF-IDF vector for a single sentence.\"\"\"\n",
        "        counter = Counter(sentence.split())\n",
        "        row_sum = 0\n",
        "        tfidf_vector = np.zeros(self.n_vocab)\n",
        "\n",
        "        for j in range(self.n_vocab):\n",
        "            word = self.index_to_word[j]\n",
        "            # term frequency only caculated from freq on the sentence\n",
        "            # same as BoW (sklearn references)\n",
        "            tf = counter.get(word, 0)\n",
        "            df = self.document_frequency.get(word, 0)\n",
        "            # references: https://scikit-learn.org/1.5/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html\n",
        "            # adding 1 on n and df -> smoothing\n",
        "            idf = np.log((1 + self.n_docs) / (1 + df)) + 1\n",
        "            tf_idf = tf * idf\n",
        "\n",
        "            row_sum += np.square(tf_idf)\n",
        "            tfidf_vector[j] = tf_idf\n",
        "\n",
        "        l2_norm = np.sqrt(row_sum)\n",
        "        if l2_norm > 0:\n",
        "            tfidf_vector /= l2_norm\n",
        "\n",
        "        return tfidf_vector\n",
        "\n",
        "    def fit(self, data):\n",
        "        \"\"\"Fits the model on the data\"\"\"\n",
        "        cleaned_data = self._clean_data(data)\n",
        "        self.n_docs = len(cleaned_data)\n",
        "        self._build_vocab(cleaned_data)\n",
        "        self._calculate_document_frequency(cleaned_data)\n",
        "        return self\n",
        "\n",
        "    def transform(self, data):\n",
        "        \"\"\"Transforms new data based on the already fitted vocabulary and document frequencies.\"\"\"\n",
        "        if self.word_to_index is None or self.document_frequency is None:\n",
        "            raise ValueError(\"The TFIDF model must be fitted before calling transform.\")\n",
        "\n",
        "        cleaned_data = self._clean_data(data)\n",
        "        transformed_matrix = np.array([self._calculate_tfidf(sentence) for sentence in cleaned_data])\n",
        "        return transformed_matrix\n",
        "\n",
        "    def fit_transform(self, data):\n",
        "        self.fit(data)\n",
        "        return self.transform(data)"
      ],
      "metadata": {
        "id": "N5h3cOWBfNe9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# split datset\n",
        "import random\n",
        "\n",
        "\n",
        "temp = []\n",
        "for index in range(len(stemming_lyrics)):\n",
        "  temp.append({\n",
        "      \"lyrics\": stemming_lyrics[index],\n",
        "      \"label\": label[index]\n",
        "  })\n",
        "\n",
        "random.shuffle(temp)\n",
        "\n",
        "train_data = temp[:int(0.8 * len(temp))]\n",
        "# train_data = temp\n",
        "test_data = temp[int(0.8 * len(temp)):]\n",
        "\n",
        "\n",
        "train_lyrics = [data[\"lyrics\"] for data in train_data]\n",
        "train_label = [data[\"label\"] for data in train_data]\n",
        "\n",
        "test_lyrics = [data[\"lyrics\"] for data in test_data]\n",
        "test_label = [data[\"label\"] for data in test_data]"
      ],
      "metadata": {
        "id": "jdU8dAUjqhBY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultinomialNaiveBayes:\n",
        "    def fit(self, X, y):\n",
        "        self.classes = np.unique(y)\n",
        "        self.class_priors = {}\n",
        "        self.feature_likelihoods = {}\n",
        "        self.join_log_likelihoods = []\n",
        "\n",
        "        # for each class calculate prior and likelihoods\n",
        "        for cls in self.classes:\n",
        "            X_cls = X[y == cls]\n",
        "\n",
        "            # class prior = total this class / total dataset\n",
        "            self.class_priors[cls] = X_cls.shape[0] / X.shape[0]\n",
        "\n",
        "            # feature likelihoods = array with cols eq to total feature\n",
        "            # for each feature, sum all occurrence of that feature\n",
        "            # divide it by (sum features in that class + total feature)\n",
        "            self.feature_likelihoods[cls] = (np.sum(X_cls, axis=0) + 1) / (np.sum(X_cls) + X.shape[1])\n",
        "\n",
        "    def predict(self, X):\n",
        "        predictions = []\n",
        "\n",
        "        # for each data\n",
        "        for x in X:\n",
        "            posteriors = {}\n",
        "\n",
        "\n",
        "            # for each class\n",
        "            for cls in self.classes:\n",
        "\n",
        "                # normal\n",
        "                # posterior = (self.class_priors[cls])\n",
        "                # posterior *= np.prod(np.power(self.feature_likelihoods[cls], x))\n",
        "\n",
        "                # problem with normal is too small result, because fraction * fraction\n",
        "                # for example 0.3 * 0.5 * ......\n",
        "                # log version\n",
        "                log_posterior = np.log(self.class_priors[cls])\n",
        "\n",
        "                # math time\n",
        "                # log(A^B) = log(A) * B\n",
        "                # log(likelihoods ^ x ) = log(likelihoods) * x\n",
        "                # log(a*b) = log(a) + log(b), then:\n",
        "                # np.prod => np.sum\n",
        "                log_posterior += np.sum(np.log(self.feature_likelihoods[cls]) * x)\n",
        "\n",
        "                # add to class posterior\n",
        "                posteriors[cls] = log_posterior\n",
        "\n",
        "            # predict the class by choosing the highst log posterior\n",
        "            self.join_log_likelihoods.append(list(posteriors.values()))\n",
        "            predictions.append(max(posteriors, key=posteriors.get))\n",
        "            # same as np.argmax(list(posteriors.values()))\n",
        "\n",
        "        return np.array(predictions)"
      ],
      "metadata": {
        "id": "rCgCaoQRtrB-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_clean_data = [' '.join(sentence) for sentence in stemming_lyrics]\n",
        "tfidf = TFIDF()\n",
        "X_train = tfidf.fit_transform(train_clean_data)\n",
        "X_train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7DhhdNFhrKhu",
        "outputId": "17b89a19-5a9f-48c3-a59f-f5a7e30e9d95"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       ...,\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_clean_data = [' '.join(sentence) for sentence in test_lyrics]\n",
        "X_test = tfidf.transform(test_clean_data)\n",
        "X_test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ppkmxwmltMnp",
        "outputId": "aad95c43-b585-4984-b305-010ab36e1b37"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       ...,\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mnb = MultinomialNaiveBayes()\n",
        "mnb.fit(X_train, label)"
      ],
      "metadata": {
        "id": "KiyX90fXuMuM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mnb.predict(X_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lfA09Va3uTjV",
        "outputId": "52ccf6fd-c9fe-411f-a99b-b975a06476b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0,\n",
              "       0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1,\n",
              "       0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
              "       1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0,\n",
              "       0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0,\n",
              "       0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def count_accuracy(y_test, y_pred):\n",
        "  #jumlah prediksi akurat / total prediksi\n",
        "  total_prediction = len(y_test)\n",
        "  total_correct = 0\n",
        "\n",
        "  for i in range(total_prediction):\n",
        "    if y_test[i] == y_pred[i]:\n",
        "\n",
        "      total_correct += 1\n",
        "\n",
        "  return total_correct / total_prediction"
      ],
      "metadata": {
        "id": "uZhtOFGRzbYy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "count_accuracy(test_label, mnb.predict(X_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8nHa8dLbzdSz",
        "outputId": "e328b0af-e366-47e9-be1b-5d6d1215c40e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8251748251748252"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from cvxopt import matrix, solvers\n",
        "\n",
        "class SVM:\n",
        "    def __init__(self, C=1, tol=1e-4):\n",
        "        self.C = C\n",
        "        self.tol = tol\n",
        "        self.w = None\n",
        "        self.b = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        n_samples, n_features = X.shape\n",
        "\n",
        "        # Ensure y is a column vector\n",
        "        y = y.reshape(-1, 1) * 1.0\n",
        "\n",
        "        # Compute Gram matrix\n",
        "        K = np.dot(X, X.T)\n",
        "\n",
        "        # Formulate QP problem\n",
        "        P = matrix(np.outer(y, y) * K)\n",
        "        q = matrix(-np.ones((n_samples, 1)))\n",
        "        A = matrix(y.T.astype(float))\n",
        "        b = matrix(np.zeros(1))\n",
        "        G = matrix(np.vstack((-np.eye(n_samples), np.eye(n_samples))))\n",
        "        h = matrix(np.vstack((np.zeros((n_samples, 1)), np.ones((n_samples, 1)) * self.C)))\n",
        "\n",
        "        # Solve QP problem\n",
        "        solvers.options['show_progress'] = False\n",
        "        solution = solvers.qp(P, q, G, h, A, b)\n",
        "        alphas = np.ravel(solution['x'])\n",
        "\n",
        "        # Identify support vectors\n",
        "        sv = alphas > self.tol\n",
        "        self.support_vectors_ = X[sv]\n",
        "        self.alphas = alphas[sv]\n",
        "        self.sv_y = y[sv]\n",
        "\n",
        "        # Calculate weights\n",
        "        self.w = np.sum(self.alphas[:, None] * self.sv_y * self.support_vectors_, axis=0)\n",
        "\n",
        "        # Calculate bias\n",
        "        self.b = np.mean(self.sv_y - np.dot(self.support_vectors_, self.w))\n",
        "\n",
        "    def predict(self, X):\n",
        "        raw_prediction = np.dot(X, self.w) + self.b\n",
        "        return np.where(raw_prediction >= 0, 1, 0)"
      ],
      "metadata": {
        "id": "Nz6G760nBD5w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y = np.array(label)\n",
        "y = np.where(y <= 0, -1, 1)"
      ],
      "metadata": {
        "id": "IofFGJHMyg9s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = SVM(C=1)\n",
        "model.fit(X_train,y)"
      ],
      "metadata": {
        "id": "DgSSVd1uo3iU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.predict(X_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-LteUefbpU3e",
        "outputId": "bad09745-d346-4f02-9edb-06dc15b3afa6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0,\n",
              "       0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1,\n",
              "       0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
              "       1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,\n",
              "       0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0,\n",
              "       0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1,\n",
              "       1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "count_accuracy(test_label, mnb.predict(X_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SNzfSScc0s_H",
        "outputId": "a986f5e9-9dcd-4bad-8452-750f02e5a591"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8251748251748252"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Calculate the accuracy of predictions.\n",
        "    Accuracy = (True Positives + True Negatives) / Total Samples\n",
        "    \"\"\"\n",
        "    correct = np.sum(y_true == y_pred)\n",
        "    total = len(y_true)\n",
        "    return correct / total\n",
        "\n",
        "\n",
        "def precision(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Calculate the precision of predictions.\n",
        "    Precision = True Positives / (True Positives + False Positives)\n",
        "    \"\"\"\n",
        "    true_positives = np.sum((y_true == 1) & (y_pred == 1))\n",
        "    predicted_positives = np.sum(y_pred == 1)\n",
        "    if predicted_positives == 0:\n",
        "        return 0  # Handle division by zero\n",
        "    return true_positives / predicted_positives\n",
        "\n",
        "\n",
        "def recall(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Calculate the recall of predictions.\n",
        "    Recall = True Positives / (True Positives + False Negatives)\n",
        "    \"\"\"\n",
        "    true_positives = np.sum((y_true == 1) & (y_pred == 1))\n",
        "    actual_positives = np.sum(y_true == 1)\n",
        "    if actual_positives == 0:\n",
        "        return 0  # Handle division by zero\n",
        "    return true_positives / actual_positives\n",
        "\n",
        "\n",
        "def f1_score(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Calculate the F1 score of predictions.\n",
        "    F1 Score = 2 * (Precision * Recall) / (Precision + Recall)\n",
        "    \"\"\"\n",
        "    p = precision(y_true, y_pred)\n",
        "    r = recall(y_true, y_pred)\n",
        "    if (p + r) == 0:\n",
        "        return 0  # Handle division by zero\n",
        "    return 2 * (p * r) / (p + r)\n"
      ],
      "metadata": {
        "id": "lJBai3TI7i52"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prediction_svm = model.predict(X_test)"
      ],
      "metadata": {
        "id": "n1KyMEQG8BfP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "def count_accuracy2(y_test, y_pred):\n",
        "    return accuracy_score(y_test, y_pred)"
      ],
      "metadata": {
        "id": "WLzB7KZv9qvw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "count_accuracy2(test_label, prediction_svm)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MUDvcWgp9uR8",
        "outputId": "f7ddd6f1-5a03-47b3-b1b4-01c708910ac4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.958041958041958"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "count_accuracy(test_label, prediction_svm)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SWdzNoTY9yjg",
        "outputId": "e825abc5-f4d5-4cc3-9197-711733147963"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.958041958041958"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_precision(y_test, y_pred):\n",
        "    # Menghitung True Positives (TP) dan False Positives (FP)\n",
        "    TP = 0\n",
        "    FP = 0\n",
        "\n",
        "    for i in range(len(y_test)):\n",
        "        if y_pred[i] == 1:  # Jika prediksi positif\n",
        "            if y_test[i] == 1:  # dan label aslinya juga positif\n",
        "                TP += 1\n",
        "            else:  # tetapi label aslinya negatif\n",
        "                FP += 1\n",
        "\n",
        "    # Menghindari pembagian dengan nol\n",
        "    if TP + FP == 0:\n",
        "        return 0\n",
        "    return TP / (TP + FP)"
      ],
      "metadata": {
        "id": "IUopFAyU96zj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "calculate_precision(test_label, prediction_svm)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6XGbKPpk-JC_",
        "outputId": "c3694eca-b8cc-480d-b8f6-93d729d20839"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9375"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_recall(y_test, y_pred):\n",
        "    # Menghitung True Positives (TP) dan False Negatives (FN)\n",
        "    TP = 0\n",
        "    FN = 0\n",
        "\n",
        "    for i in range(len(y_test)):\n",
        "        if y_test[i] == 1:  # Jika label asli positif\n",
        "            if y_pred[i] == 1:  # dan prediksi juga positif\n",
        "                TP += 1\n",
        "            else:  # tetapi prediksi negatif\n",
        "                FN += 1\n",
        "\n",
        "    # Menghindari pembagian dengan nol\n",
        "    if TP + FN == 0:\n",
        "        return 0\n",
        "    return TP / (TP + FN)\n"
      ],
      "metadata": {
        "id": "AtfXMdIk-V_Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "calculate_recall(test_label, prediction_svm)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PFk3nZRL-29c",
        "outputId": "01c91125-7f14-466f-a439-5e67d053a83a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.967741935483871"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_f1_score(y_test, y_pred):\n",
        "    # Menghitung Precision\n",
        "    TP = 0\n",
        "    FP = 0\n",
        "    FN = 0\n",
        "\n",
        "    for i in range(len(y_test)):\n",
        "        if y_pred[i] == 1:  # Jika prediksi positif\n",
        "            if y_test[i] == 1:  # dan label asli positif\n",
        "                TP += 1\n",
        "            else:  # tetapi label asli negatif\n",
        "                FP += 1\n",
        "        elif y_test[i] == 1:  # jika label asli positif dan prediksi negatif\n",
        "            FN += 1\n",
        "\n",
        "    precision = TP / (TP + FP) if (TP + FP) != 0 else 0\n",
        "    recall = TP / (TP + FN) if (TP + FN) != 0 else 0\n",
        "\n",
        "    # Menghindari pembagian dengan nol untuk F1 Score\n",
        "    if precision + recall == 0:\n",
        "        return 0\n",
        "    return 2 * (precision * recall) / (precision + recall)\n"
      ],
      "metadata": {
        "id": "6vXyZ_ni_EXX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "calculate_f1_score(test_label, prediction_svm)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3jRxESLc_QQw",
        "outputId": "1062bbf2-63fb-428b-b84e-81bf1f0a5080"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9523809523809523"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prediction_mnb = mnb.predict(X_test)"
      ],
      "metadata": {
        "id": "nzMMRKZM_dgC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_mnb = count_accuracy(test_label, prediction_mnb)\n",
        "precision_mnb = calculate_precision(test_label, prediction_mnb)\n",
        "recall_mnb = calculate_recall(test_label, prediction_mnb)\n",
        "f1_score_mnb = calculate_f1_score(test_label, prediction_mnb)\n",
        "\n",
        "print(f\"Accuracy: {accuracy_mnb}\")\n",
        "print(f\"Precision: {precision_mnb}\")\n",
        "print(f\"Recall: {recall_mnb}\")\n",
        "print(f\"F1 Score: {f1_score_mnb}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j9vSuV8o_hlW",
        "outputId": "c6e665ae-9d76-4e5c-e1c9-c440fbfb4205"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.8251748251748252\n",
            "Precision: 0.9743589743589743\n",
            "Recall: 0.6129032258064516\n",
            "F1 Score: 0.7524752475247525\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "\n",
        "joblib.dump(model, 'model_svm.h5')\n",
        "joblib.dump(mnb, 'model_mnb.h5')\n",
        "joblib.dump(tfidf, 'tfidf.h5')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V6Lp1fPCAFdg",
        "outputId": "347f2c55-1c62-438f-92c8-18010f44bf12"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['tfidf.h5']"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "with open('model_mnb.pkl', 'wb') as file:\n",
        "    pickle.dump(mnb, file)"
      ],
      "metadata": {
        "id": "vB6ylItnWvjI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('tfidf_vector.pkl', 'wb') as file:\n",
        "    pickle.dump(tfidf, file)"
      ],
      "metadata": {
        "id": "Nwoa7t9-aAhr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('model_svm.pkl', 'wb') as file:\n",
        "    pickle.dump(model, file)"
      ],
      "metadata": {
        "id": "YiGqWNvw72eF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('tfidf_vector.pkl', 'rb') as file:\n",
        "    test_tfidf = pickle.load(file)"
      ],
      "metadata": {
        "id": "cgYgatBB7tIM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('tfidf_vector.pkl', 'rb') as file:\n",
        "    test_tfidf = pickle.load(file)"
      ],
      "metadata": {
        "id": "Xe0PLoNEaKV-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "r = test_tfidf.transform(test_clean_data)\n",
        "r"
      ],
      "metadata": {
        "id": "DPD8MXtuaSS9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('model_mnb.pkl', 'rb') as file:\n",
        "    test_mnb = pickle.load(file)"
      ],
      "metadata": {
        "id": "zqJpohBma6le"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "te = test_mnb.predict(r)\n",
        "te"
      ],
      "metadata": {
        "id": "UjR_nv4_bA4x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('model_svm.pkl', 'wb') as file:\n",
        "    test_svm = pickle.dump(model, file)"
      ],
      "metadata": {
        "id": "-B89t09-ht0c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('model_svm.pkl')"
      ],
      "metadata": {
        "id": "5DmlwZUsh3nt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "81cwRmUK2lrK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}